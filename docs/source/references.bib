@ARTICLE{Stringer2021-cx,
  title     = "Cellpose: a generalist algorithm for cellular segmentation",
  author    = "Stringer, Carsen and Wang, Tim and Michaelos, Michalis and
               Pachitariu, Marius",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  18,
  number    =  1,
  pages     = "100--106",
  abstract  = "Many biological applications require the segmentation of cell
               bodies, membranes and nuclei from microscopy images. Deep
               learning has enabled great progress on this problem, but current
               methods are specialized for images that have large training
               datasets. Here we introduce a generalist, deep learning-based
               segmentation method called Cellpose, which can precisely segment
               cells from a wide range of image types and does not require model
               retraining or parameter adjustments. Cellpose was trained on a
               new dataset of highly varied images of cells, containing over
               70,000 segmented objects. We also demonstrate a three-dimensional
               (3D) extension of Cellpose that reuses the two-dimensional (2D)
               model and does not require 3D-labeled data. To support community
               contributions to the training data, we developed software for
               manual labeling and for curation of the automated results.
               Periodically retraining the model on the community-contributed
               data will ensure that Cellpose improves constantly.",
  month     =  jan,
  year      =  2021,
  url       = "http://dx.doi.org/10.1038/s41592-020-01018-x",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41592-020-01018-x",
  language  = "en"
}

@MISC{UnknownUnknown-wz,
  title        = "{SPIDER} - models and datasets - a histai Collection",
  abstract     = "Patch-based multi class classification models and the
                  associated datasets",
  howpublished = "\url{https://huggingface.co/collections/histai/spider-models-and-datasets-6814834eca365b006389c117}",
  note         = "Accessed: 2025-7-20",
  keywords     = "LazySlide-doc-citations"
}

@ARTICLE{Haghighat2022-sy,
  title     = "Automated quality assessment of large digitised histology cohorts
               by artificial intelligence",
  author    = "Haghighat, Maryam and Browning, Lisa and Sirinukunwattana, Korsuk
               and Malacrino, Stefano and Khalid Alham, Nasullah and Colling,
               Richard and Cui, Ying and Rakha, Emad and Hamdy, Freddie C and
               Verrill, Clare and Rittscher, Jens",
  journal   = "Sci. Rep.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  12,
  number    =  1,
  pages     =  5002,
  abstract  = "Research using whole slide images (WSIs) of histopathology slides
               has increased exponentially over recent years. Glass slides from
               retrospective cohorts, some with patient follow-up data are
               digitised for the development and validation of artificial
               intelligence (AI) tools. Such resources, therefore, become very
               important, with the need to ensure that their quality is of the
               standard necessary for downstream AI development. However, manual
               quality control of large cohorts of WSIs by visual assessment is
               unfeasible, and whilst quality control AI algorithms exist, these
               focus on bespoke aspects of image quality, e.g. focus, or use
               traditional machine-learning methods, which are unable to
               classify the range of potential image artefacts that should be
               considered. In this study, we have trained and validated a
               multi-task deep neural network to automate the process of quality
               control of a large retrospective cohort of prostate cases from
               which glass slides have been scanned several years after
               production, to determine both the usability of the images at the
               diagnostic level (considered in this study to be the minimal
               standard for research) and the common image artefacts present.
               Using a two-layer approach, quality overlays of WSIs were
               generated from a quality assessment (QA) undertaken at
               patch-level at [Formula: see text] magnification. From these
               quality overlays the slide-level quality scores were predicted
               and then compared to those generated by three specialist
               urological pathologists, with a Pearson correlation of 0.89 for
               overall 'usability' (at a diagnostic level), and 0.87 and 0.82
               for focus and H\&E staining quality scores respectively. To
               demonstrate its wider potential utility, we subsequently applied
               our QA pipeline to the TCGA prostate cancer cohort and to a
               colorectal cancer cohort, for comparison. Our model, designated
               as PathProfiler, indicates comparable predicted usability of
               images from the cohorts assessed (86-90\% of WSIs predicted to be
               usable), and perhaps more significantly is able to predict WSIs
               that could benefit from an intervention such as re-scanning or
               re-staining for quality improvement. We have shown in this study
               that AI can be used to automate the process of quality control of
               large retrospective WSI cohorts to maximise their utility for
               research.",
  month     =  "23~" # mar,
  year      =  2022,
  url       = "http://dx.doi.org/10.1038/s41598-022-08351-5",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41598-022-08351-5",
  language  = "en"
}

@ARTICLE{Wang2020-ku,
  title         = "{FocusLiteNN}: High efficiency Focus Quality Assessment for
                   digital pathology",
  author        = "Wang, Zhongling and Hosseini, Mahdi S and Miles, Adyn and
                   Plataniotis, Konstantinos N and Wang, Zhou",
  journal       = "arXiv [eess.IV]",
  abstract      = "Out-of-focus microscopy lens in digital pathology is a
                   critical bottleneck in high-throughput Whole Slide Image
                   (WSI) scanning platforms, for which pixel-level automated
                   Focus Quality Assessment (FQA) methods are highly desirable
                   to help significantly accelerate the clinical workflows.
                   Existing FQA methods include both knowledge-driven and
                   data-driven approaches. While data-driven approaches such as
                   Convolutional Neural Network (CNN) based methods have shown
                   great promises, they are difficult to use in practice due to
                   their high computational complexity and lack of
                   transferability. Here, we propose a highly efficient
                   CNN-based model that maintains fast computations similar to
                   the knowledge-driven methods without excessive hardware
                   requirements such as GPUs. We create a training dataset using
                   FocusPath which encompasses diverse tissue slides across nine
                   different stain colors, where the stain diversity greatly
                   helps the model to learn diverse color spectrum and tissue
                   structures. In our attempt to reduce the CNN complexity, we
                   find with surprise that even trimming down the CNN to the
                   minimal level, it still achieves a highly competitive
                   performance. We introduce a novel comprehensive evaluation
                   dataset, the largest of its kind, annotated and compiled from
                   TCGA repository for model assessment and comparison, for
                   which the proposed method exhibits superior precision-speed
                   trade-off when compared with existing knowledge-driven and
                   data-driven FQA approaches.",
  month         =  "11~" # jul,
  year          =  2020,
  url           = "http://arxiv.org/abs/2007.06565",
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Wang2024-jb,
  title     = "A pathology foundation model for cancer diagnosis and prognosis
               prediction",
  author    = "Wang, Xiyue and Zhao, Junhan and Marostica, Eliana and Yuan, Wei
               and Jin, Jietian and Zhang, Jiayu and Li, Ruijiang and Tang,
               Hongping and Wang, Kanran and Li, Yu and Wang, Fang and Peng,
               Yulong and Zhu, Junyou and Zhang, Jing and Jackson, Christopher R
               and Zhang, Jun and Dillon, Deborah and Lin, Nancy U and Sholl,
               Lynette and Denize, Thomas and Meredith, David and Ligon, Keith L
               and Signoretti, Sabina and Ogino, Shuji and Golden, Jeffrey A and
               Nasrallah, Maclean P and Han, Xiao and Yang, Sen and Yu,
               Kun-Hsing",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  634,
  number    =  8035,
  pages     = "970--978",
  abstract  = "Histopathology image evaluation is indispensable for cancer
               diagnoses and subtype classification. Standard artificial
               intelligence methods for histopathology image analyses have
               focused on optimizing specialized models for each diagnostic
               task1,2. Although such methods have achieved some success, they
               often have limited generalizability to images generated by
               different digitization protocols or samples collected from
               different populations3. Here, to address this challenge, we
               devised the Clinical Histopathology Imaging Evaluation Foundation
               (CHIEF) model, a general-purpose weakly supervised machine
               learning framework to extract pathology imaging features for
               systematic cancer evaluation. CHIEF leverages two complementary
               pretraining methods to extract diverse pathology representations:
               unsupervised pretraining for tile-level feature identification
               and weakly supervised pretraining for whole-slide pattern
               recognition. We developed CHIEF using 60,530 whole-slide images
               spanning 19 anatomical sites. Through pretraining on 44 terabytes
               of high-resolution pathology imaging datasets, CHIEF extracted
               microscopic representations useful for cancer cell detection,
               tumour origin identification, molecular profile characterization
               and prognostic prediction. We successfully validated CHIEF using
               19,491 whole-slide images from 32 independent slide sets
               collected from 24 hospitals and cohorts internationally. Overall,
               CHIEF outperformed the state-of-the-art deep learning methods by
               up to 36.1\%, showing its ability to address domain shifts
               observed in samples from diverse populations and processed by
               different slide preparation methods. CHIEF provides a
               generalizable foundation for efficient digital pathology
               evaluation for patients with cancer.",
  month     =  "4~" # oct,
  year      =  2024,
  url       = "http://dx.doi.org/10.1038/s41586-024-07894-z",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41586-024-07894-z",
  language  = "en"
}

@ARTICLE{Wang2022-rk,
  title     = "Transformer-based unsupervised contrastive learning for
               histopathological image classification",
  author    = "Wang, Xiyue and Yang, Sen and Zhang, Jun and Wang, Minghui and
               Zhang, Jing and Yang, Wei and Huang, Junzhou and Han, Xiao",
  journal   = "Med. Image Anal.",
  publisher = "Elsevier BV",
  volume    =  81,
  number    =  102559,
  pages     =  102559,
  abstract  = "A large-scale and well-annotated dataset is a key factor for the
               success of deep learning in medical image analysis. However,
               assembling such large annotations is very challenging, especially
               for histopathological images with unique characteristics (e.g.,
               gigapixel image size, multiple cancer types, and wide staining
               variations). To alleviate this issue, self-supervised learning
               (SSL) could be a promising solution that relies only on unlabeled
               data to generate informative representations and generalizes well
               to various downstream tasks even with limited annotations. In
               this work, we propose a novel SSL strategy called
               semantically-relevant contrastive learning (SRCL), which compares
               relevance between instances to mine more positive pairs. Compared
               to the two views from an instance in traditional contrastive
               learning, our SRCL aligns multiple positive instances with
               similar visual concepts, which increases the diversity of
               positives and then results in more informative representations.
               We employ a hybrid model (CTransPath) as the backbone, which is
               designed by integrating a convolutional neural network (CNN) and
               a multi-scale Swin Transformer architecture. The CTransPath is
               pretrained on massively unlabeled histopathological images that
               could serve as a collaborative local-global feature extractor to
               learn universal feature representations more suitable for tasks
               in the histopathology image domain. The effectiveness of our
               SRCL-pretrained CTransPath is investigated on five types of
               downstream tasks (patch retrieval, patch classification,
               weakly-supervised whole-slide image classification, mitosis
               detection, and colorectal adenocarcinoma gland segmentation),
               covering nine public datasets. The results show that our
               SRCL-based visual representations not only achieve
               state-of-the-art performance in each dataset, but are also more
               robust and transferable than other SSL methods and ImageNet
               pretraining (both supervised and self-supervised methods). Our
               code and pretrained model are available at
               https://github.com/Xiyue-Wang/TransPath.",
  month     =  "1~" # oct,
  year      =  2022,
  url       = "http://dx.doi.org/10.1016/j.media.2022.102559",
  keywords  = "Feature extraction; Histopathology; Self-supervised learning;
               Transformer;LazySlide-doc-citations",
  doi       = "10.1016/j.media.2022.102559",
  language  = "en"
}

@ARTICLE{Chen2025-ok,
  title     = "A visual-omics foundation model to bridge histopathology with
               spatial transcriptomics",
  author    = "Chen, Weiqing and Zhang, Pengzhi and Tran, Tu N and Xiao, Yiwei
               and Li, Shengyu and Shah, Vrutant V and Cheng, Hao and Brannan,
               Kristopher W and Youker, Keith and Lai, Li and Fang, Longhou and
               Yang, Yu and Le, Nhat-Tu and Abe, Jun-Ichi and Chen, Shu-Hsia and
               Ma, Qin and Chen, Ken and Song, Qianqian and Cooke, John P and
               Wang, Guangyu",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  22,
  number    =  7,
  pages     = "1568--1582",
  abstract  = "Artificial intelligence has revolutionized computational biology.
               Recent developments in omics technologies, including single-cell
               RNA sequencing and spatial transcriptomics, provide detailed
               genomic data alongside tissue histology. However, current
               computational models focus on either omics or image analysis,
               lacking their integration. To address this, we developed OmiCLIP,
               a visual-omics foundation model linking hematoxylin and eosin
               images and transcriptomics using tissue patches from Visium data.
               We transformed transcriptomic data into 'sentences' by
               concatenating top-expressed gene symbols from each patch. We
               curated a dataset of 2.2 million paired tissue images and
               transcriptomic data across 32 organs to train OmiCLIP integrating
               histology and transcriptomics. Building on OmiCLIP, our Loki
               platform offers five key functions: tissue alignment, annotation
               via bulk RNA sequencing or marker genes, cell-type decomposition,
               image-transcriptomics retrieval and spatial transcriptomics gene
               expression prediction from hematoxylin and eosin-stained images.
               Compared with 22 state-of-the-art models on 5 simulations, and 19
               public and 4 in-house experimental datasets, Loki demonstrated
               consistent accuracy and robustness.",
  month     =  "29~" # jul,
  year      =  2025,
  url       = "http://dx.doi.org/10.1038/s41592-025-02707-1",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41592-025-02707-1",
  language  = "en"
}

@ARTICLE{Nechaev2024-wi,
  title         = "Hibou: A family of foundational vision transformers for
                   pathology",
  author        = "Nechaev, Dmitry and Pchelnikov, Alexey and Ivanova, Ekaterina",
  journal       = "arXiv [eess.IV]",
  abstract      = "Pathology, the microscopic examination of diseased tissue, is
                   critical for diagnosing various medical conditions,
                   particularly cancers. Traditional methods are labor-intensive
                   and prone to human error. Digital pathology, which converts
                   glass slides into high-resolution digital images for analysis
                   by computer algorithms, revolutionizes the field by enhancing
                   diagnostic accuracy, consistency, and efficiency through
                   automated image analysis and large-scale data processing.
                   Foundational transformer pretraining is crucial for
                   developing robust, generalizable models as it enables
                   learning from vast amounts of unannotated data. This paper
                   introduces the Hibou family of foundational vision
                   transformers for pathology, leveraging the DINOv2 framework
                   to pretrain two model variants, Hibou-B and Hibou-L, on a
                   proprietary dataset of over 1 million whole slide images
                   (WSIs) representing diverse tissue types and staining
                   techniques. Our pretrained models demonstrate superior
                   performance on both patch-level and slide-level benchmarks,
                   surpassing existing state-of-the-art methods. Notably,
                   Hibou-L achieves the highest average accuracy across multiple
                   benchmark datasets. To support further research and
                   application in the field, we have open-sourced the Hibou
                   models, which can be accessed at
                   https://github.com/HistAI/hibou.",
  month         =  "7~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.05074",
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Karasikov2025-wp,
  title         = "Training state-of-the-art pathology foundation models with
                   orders of magnitude less data",
  author        = "Karasikov, Mikhail and van Doorn, Joost and Känzig, Nicolas
                   and Cesur, Melis Erdal and Horlings, Hugo Mark and Berke,
                   Robert and Tang, Fei and Otálora, Sebastian",
  journal       = "arXiv [cs.CV]",
  abstract      = "The field of computational pathology has recently seen rapid
                   advances driven by the development of modern vision
                   foundation models (FMs), typically trained on vast
                   collections of pathology images. Recent studies demonstrate
                   that increasing the training data set and model size and
                   integrating domain-specific image processing techniques can
                   significantly enhance the model's performance on downstream
                   tasks. Building on these insights, our work incorporates
                   several recent modifications to the standard DINOv2 framework
                   from the literature to optimize the training of pathology
                   FMs. We also apply a post-training procedure for fine-tuning
                   models on higher-resolution images to further enrich the
                   information encoded in the embeddings. We present three novel
                   pathology FMs trained on up to two orders of magnitude fewer
                   WSIs than those used to train other state-of-the-art FMs
                   while demonstrating a comparable or superior performance on
                   downstream tasks. Even the model trained on TCGA alone (12k
                   WSIs) outperforms most existing FMs and, on average, matches
                   Virchow2, the second-best FM published to date. This suggests
                   that there still remains a significant potential for further
                   improving the models and algorithms used to train pathology
                   FMs to take full advantage of the vast data collections.",
  month         =  "7~" # apr,
  year          =  2025,
  url           = "http://arxiv.org/abs/2504.05186",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Goldsborough2024-oc,
  title         = "{InstanSeg}: an embedding-based instance segmentation
                   algorithm optimized for accurate, efficient and portable cell
                   segmentation",
  author        = "Goldsborough, Thibaut and Philps, Ben and O'Callaghan, Alan
                   and Inglis, Fiona and Leplat, Leo and Filby, Andrew and
                   Bilen, Hakan and Bankhead, Peter",
  journal       = "arXiv [cs.CV]",
  abstract      = "Cell and nucleus segmentation are fundamental tasks for
                   quantitative bioimage analysis. Despite progress in recent
                   years, biologists and other domain experts still require
                   novel algorithms to handle increasingly large and complex
                   real-world datasets. These algorithms must not only achieve
                   state-of-the-art accuracy, but also be optimized for
                   efficiency, portability and user-friendliness. Here, we
                   introduce InstanSeg: a novel embedding-based instance
                   segmentation pipeline designed to identify cells and nuclei
                   in microscopy images. Using six public cell segmentation
                   datasets, we demonstrate that InstanSeg can significantly
                   improve accuracy when compared to the most widely used
                   alternative methods, while reducing the processing time by at
                   least 60\%. Furthermore, InstanSeg is designed to be fully
                   serializable as TorchScript and supports GPU acceleration on
                   a range of hardware. We provide an open-source implementation
                   of InstanSeg in Python, in addition to a user-friendly,
                   interactive QuPath extension for inference written in Java.
                   Our code and pre-trained models are available at
                   https://github.com/instanseg/instanseg .",
  month         =  "28~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.15954",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Tommasino2024-tg,
  title         = "{NuLite} -- lightweight and Fast model for nuclei instance
                   segmentation and classification",
  author        = "Tommasino, Cristian and Russo, Cristiano and Rinaldi, Antonio
                   Maria",
  journal       = "arXiv [eess.IV]",
  abstract      = "In pathology, accurate and efficient analysis of Hematoxylin
                   and Eosin (H\&E) slides is crucial for timely and effective
                   cancer diagnosis. Although many deep learning solutions for
                   nuclei instance segmentation and classification exist in the
                   literature, they often entail high computational costs and
                   resource requirements, thus limiting their practical usage in
                   medical applications. To address this issue, we introduce a
                   novel convolutional neural network, NuLite, a U-Net-like
                   architecture designed explicitly on Fast-ViT, a
                   state-of-the-art (SOTA) lightweight CNN. We obtained three
                   versions of our model, NuLite-S, NuLite-M, and NuLite-H,
                   trained on the PanNuke dataset. The experimental results
                   prove that our models equal CellViT (SOTA) in terms of
                   panoptic quality and detection. However, our lightest model,
                   NuLite-S, is 40 times smaller in terms of parameters and
                   about 8 times smaller in terms of GFlops, while our heaviest
                   model is 17 times smaller in terms of parameters and about 7
                   times smaller in terms of GFlops. Moreover, our model is up
                   to about 8 times faster than CellViT. Lastly, to prove the
                   effectiveness of our solution, we provide a robust comparison
                   of external datasets, namely CoNseP, MoNuSeg, and GlySAC. Our
                   model is publicly available at
                   https://github.com/CosmoIknosLab/NuLite",
  month         =  "3~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.01797",
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Filiot2025-bn,
  title         = "Distilling foundation models for robust and efficient models
                   in digital pathology",
  author        = "Filiot, Alexandre and Dop, Nicolas and Tchita, Oussama and
                   Riou, Auriane and Dubois, Rémy and Peeters, Thomas and
                   Valter, Daria and Scalbert, Marin and Saillard, Charlie and
                   Robin, Geneviève and Olivier, Antoine",
  journal       = "arXiv [cs.CV]",
  abstract      = "In recent years, the advent of foundation models (FM) for
                   digital pathology has relied heavily on scaling the
                   pre-training datasets and the model size, yielding large and
                   powerful models. While it resulted in improving the
                   performance on diverse downstream tasks, it also introduced
                   increased computational cost and inference time. In this
                   work, we explore the distillation of a large foundation model
                   into a smaller one, reducing the number of parameters by
                   several orders of magnitude. Leveraging distillation
                   techniques, our distilled model, H0-mini, achieves nearly
                   comparable performance to large FMs at a significantly
                   reduced inference cost. It is evaluated on several public
                   benchmarks, achieving 3rd place on the HEST benchmark and 5th
                   place on the EVA benchmark. Additionally, a robustness
                   analysis conducted on the PLISM dataset demonstrates that our
                   distilled model reaches excellent robustness to variations in
                   staining and scanning conditions, significantly outperforming
                   other state-of-the art models. This opens new perspectives to
                   design lightweight and robust models for digital pathology,
                   without compromising on performance.",
  month         =  "27~" # jan,
  year          =  2025,
  url           = "http://arxiv.org/abs/2501.16239",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  keywords      = "LazySlide-doc-citations"
}

@MISC{Bioptimus2025-lj,
  title    = "{H}-optimus-1",
  author   = "{Bioptimus}",
  year     =  2025,
  url      = "https://huggingface.co/bioptimus/H-optimus-1",
  keywords = "LazySlide-doc-citations"
}

@MISC{Saillard2024-ho,
  title    = "{H}-optimus-0",
  author   = "Saillard, Charlie and Jenatton, Rodolphe and Llinares-López,
              Felipe and Mariet, Zelda and Cahané, David and Durand, Eric and
              Vert, Jean-Philippe",
  year     =  2024,
  url      = "https://github.com/bioptimus/releases/tree/main/models/h-optimus/v0",
  keywords = "LazySlide-doc-citations"
}

@ARTICLE{Filiot2024-at,
  title         = "Phikon-{v2}, A large and public feature extractor for
                   biomarker prediction",
  author        = "Filiot, Alexandre and Jacob, Paul and Mac Kain, Alice and
                   Saillard, Charlie",
  journal       = "arXiv [eess.IV]",
  abstract      = "Gathering histopathology slides from over 100 publicly
                   available cohorts, we compile a diverse dataset of 460
                   million pathology tiles covering more than 30 cancer sites.
                   Using this dataset, we train a large self-supervised vision
                   transformer using DINOv2 and publicly release one iteration
                   of this model for further experimentation, coined Phikon-v2.
                   While trained on publicly available histology slides,
                   Phikon-v2 surpasses our previously released model (Phikon)
                   and performs on par with other histopathology foundation
                   models (FM) trained on proprietary data. Our benchmarks
                   include eight slide-level tasks with results reported on
                   external validation cohorts avoiding any data contamination
                   between pre-training and evaluation datasets. Our downstream
                   training procedure follows a simple yet robust ensembling
                   strategy yielding a +1.75 AUC increase across tasks and
                   models compared to one-shot retraining (p<0.001). We compare
                   Phikon (ViT-B) and Phikon-v2 (ViT-L) against 14 different
                   histology feature extractors, making our evaluation the most
                   comprehensive to date. Our result support evidences that
                   DINOv2 handles joint model and data scaling better than iBOT.
                   Also, we show that recent scaling efforts are overall
                   beneficial to downstream performance in the context of
                   biomarker prediction with GigaPath and H-Optimus-0 (two ViT-g
                   with 1.1B parameters each) standing out. However, the
                   statistical margins between the latest top-performing FMs
                   remain mostly non-significant; some even underperform on
                   specific indications or tasks such as MSI prediction -
                   deposed by a 13x smaller model developed internally. While
                   latest foundation models may exhibit limitations for clinical
                   deployment, they nonetheless offer excellent grounds for the
                   development of more specialized and cost-efficient histology
                   encoders fueling AI-guided diagnostic tools.",
  month         =  "13~" # sep,
  year          =  2024,
  url           = "http://arxiv.org/abs/2409.09173",
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Filiot2023-vg,
  title    = "Scaling self-Supervised Learning for histopathology with Masked
              Image Modeling",
  author   = "Filiot, Alexandre and Ghermi, Ridouane and Olivier, Antoine and
              Jacob, Paul and Fidon, Lucas and Camara, Axel and Mac Kain, Alice
              and Saillard, Charlie and Schiratti, Jean-Baptiste",
  journal  = "medRxiv",
  pages    = "2023.07.21.23292757",
  abstract = "Computational pathology is revolutionizing the field of pathology
              by integrating advanced computer vision and machine learning
              technologies into diagnostic workflows. It offers unprecedented
              opportunities for improved efficiency in treatment decisions by
              allowing pathologists to achieve higher precision and objectivity
              in disease classification, tumor microenvironment description and
              identification of new biomarkers. However, the potential of
              computational pathology in personalized medicine comes with
              significant challenges, particularly in annotating whole slide
              images (WSI), which is time-consuming, costly and subject to
              inter-observer variability. To address these challenges,
              Self-Supervised Learning (SSL) has emerged as a promising solution
              to learn representations from histology patches and leverage large
              volumes of unlabelled WSI. Recently, Masked Image Modeling (MIM)
              as a SSL framework has emerged and is now considered to outperform
              purely contrastive learning paradigms. In this work, we therefore
              explore the application of MIM to histology using iBOT, a
              self-supervised transformer-based framework. Through a wide range
              of 17 downstream tasks over seven cancer indications, both at the
              slide and patch levels, we provide recommendations on the
              pre-training of large models for histology data using MIM. First,
              we demonstrate that in-domain pre-training with iBOT outperforms
              both ImageNet pre-training and a model pre-trained with a purely
              contrastive learning objective, MoCo v2. Second, we show that
              Vision Transformers (ViT) models, when scaled appropriately, have
              the capability to learn pan-cancer representations that benefit a
              large variety of downstream tasks. Finally, our iBOT ViT-Base
              model (80 million parameters), pre-trained on more than 40 million
              histology images from 16 different cancer types, achieves
              state-of-the-art performance in most weakly-supervised WSI
              classification tasks compared to other SSL frameworks available in
              the literature. This paves the way for the development of a
              foundation model for histopathology. Our code, models and features
              are publicly available athttps://github.com/owkin/HistoSSLscaling.",
  month    =  "26~" # jul,
  year     =  2023,
  url      = "https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v3.abstract",
  keywords = "LazySlide-doc-citations",
  doi      = "10.1101/2023.07.21.23292757",
  language = "en"
}

@ARTICLE{Zimmermann2024-ya,
  title         = "{Virchow2}: Scaling self-supervised mixed magnification
                   models in pathology",
  author        = "Zimmermann, Eric and Vorontsov, Eugene and Viret, Julian and
                   Casson, Adam and Zelechowski, Michal and Shaikovski, George
                   and Tenenholtz, Neil and Hall, James and Klimstra, David and
                   Yousfi, Razik and Fuchs, Thomas and Fusi, Nicolo and Liu,
                   Siqi and Severson, Kristen",
  journal       = "arXiv [cs.CV]",
  abstract      = "Foundation models are rapidly being developed for
                   computational pathology applications. However, it remains an
                   open question which factors are most important for downstream
                   performance with data scale and diversity, model size, and
                   training algorithm all playing a role. In this work, we
                   propose algorithmic modifications, tailored for pathology,
                   and we present the result of scaling both data and model
                   size, surpassing previous studies in both dimensions. We
                   introduce three new models: Virchow2, a 632 million parameter
                   vision transformer, Virchow2G, a 1.9 billion parameter vision
                   transformer, and Virchow2G Mini, a 22 million parameter
                   distillation of Virchow2G, each trained with 3.1 million
                   histopathology whole slide images, with diverse tissues,
                   originating institutions, and stains. We achieve state of the
                   art performance on 12 tile-level tasks, as compared to the
                   top performing competing models. Our results suggest that
                   data diversity and domain-specific methods can outperform
                   models that only scale in the number of parameters, but, on
                   average, performance benefits from the combination of
                   domain-specific methods, data scale, and model scale.",
  month         =  "1~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.00738",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Vorontsov2024-di,
  title     = "A foundation model for clinical-grade computational pathology and
               rare cancers detection",
  author    = "Vorontsov, Eugene and Bozkurt, Alican and Casson, Adam and
               Shaikovski, George and Zelechowski, Michal and Severson, Kristen
               and Zimmermann, Eric and Hall, James and Tenenholtz, Neil and
               Fusi, Nicolo and Yang, Ellen and Mathieu, Philippe and van Eck,
               Alexander and Lee, Donghun and Viret, Julian and Robert, Eric and
               Wang, Yi Kan and Kunz, Jeremy D and Lee, Matthew C H and
               Bernhard, Jan H and Godrich, Ran A and Oakley, Gerard and Millar,
               Ewan and Hanna, Matthew and Wen, Hannah and Retamero, Juan A and
               Moye, William A and Yousfi, Razik and Kanan, Christopher and
               Klimstra, David S and Rothrock, Brandon and Liu, Siqi and Fuchs,
               Thomas J",
  journal   = "Nat. Med.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  30,
  number    =  10,
  pages     = "2924--2935",
  abstract  = "The analysis of histopathology images with artificial
               intelligence aims to enable clinical decision support systems and
               precision medicine. The success of such applications depends on
               the ability to model the diverse patterns observed in pathology
               images. To this end, we present Virchow, the largest foundation
               model for computational pathology to date. In addition to the
               evaluation of biomarker prediction and cell identification, we
               demonstrate that a large foundation model enables pan-cancer
               detection, achieving 0.95 specimen-level area under the (receiver
               operating characteristic) curve across nine common and seven rare
               cancers. Furthermore, we show that with less training data, the
               pan-cancer detector built on Virchow can achieve similar
               performance to tissue-specific clinical-grade models in
               production and outperform them on some rare variants of cancer.
               Virchow's performance gains highlight the value of a foundation
               model and open possibilities for many high-impact applications
               with limited amounts of labeled training data.",
  month     =  "22~" # oct,
  year      =  2024,
  url       = "http://dx.doi.org/10.1038/s41591-024-03141-0",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41591-024-03141-0",
  language  = "en"
}

@ARTICLE{Lu2024-nu,
  title     = "A visual-language foundation model for computational pathology",
  author    = "Lu, Ming Y and Chen, Bowen and Williamson, Drew F K and Chen,
               Richard J and Liang, Ivy and Ding, Tong and Jaume, Guillaume and
               Odintsov, Igor and Le, Long Phi and Gerber, Georg and Parwani,
               Anil V and Zhang, Andrew and Mahmood, Faisal",
  journal   = "Nat. Med.",
  publisher = "Nature Publishing Group",
  volume    =  30,
  number    =  3,
  pages     = "863--874",
  abstract  = "The accelerated adoption of digital pathology and advances in
               deep learning have enabled the development of robust models for
               various pathology tasks across a diverse array of diseases and
               patient cohorts. However, model training is often difficult due
               to label scarcity in the medical domain, and a model's usage is
               limited by the specific task and disease for which it is trained.
               Additionally, most models in histopathology leverage only image
               data, a stark contrast to how humans teach each other and reason
               about histopathologic entities. We introduce CONtrastive learning
               from Captions for Histopathology (CONCH), a visual-language
               foundation model developed using diverse sources of
               histopathology images, biomedical text and, notably, over 1.17
               million image-caption pairs through task-agnostic pretraining.
               Evaluated on a suite of 14 diverse benchmarks, CONCH can be
               transferred to a wide range of downstream tasks involving
               histopathology images and/or text, achieving state-of-the-art
               performance on histology image classification, segmentation,
               captioning, and text-to-image and image-to-text retrieval. CONCH
               represents a substantial leap over concurrent visual-language
               pretrained systems for histopathology, with the potential to
               directly facilitate a wide array of machine learning-based
               workflows requiring minimal or no further supervised fine-tuning.",
  month     =  "19~" # mar,
  year      =  2024,
  url       = "http://dx.doi.org/10.1038/s41591-024-02856-4",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41591-024-02856-4",
  language  = "en"
}

@ARTICLE{Weng2024-jf,
  title     = "{GrandQC}: A comprehensive solution to quality control problem in
               digital pathology",
  author    = "Weng, Zhilong and Seper, Alexander and Pryalukhin, Alexey and
               Mairinger, Fabian and Wickenhauser, Claudia and Bauer, Marcus and
               Glamann, Lennert and Bläker, Hendrik and Lingscheidt, Thomas and
               Hulla, Wolfgang and Jonigk, Danny and Schallenberg, Simon and
               Bychkov, Andrey and Fukuoka, Junya and Braun, Martin and
               Schömig-Markiefka, Birgid and Klein, Sebastian and Thiel, Andreas
               and Bozek, Katarzyna and Netto, George J and Quaas, Alexander and
               Büttner, Reinhard and Tolkach, Yuri",
  journal   = "Nat. Commun.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  15,
  number    =  1,
  pages     =  10685,
  abstract  = "Histological slides contain numerous artifacts that can
               significantly deteriorate the performance of image analysis
               algorithms. Here we develop the GrandQC tool for tissue and
               multi-class artifact segmentation. GrandQC allows for
               high-precision tissue segmentation (Dice score 0.957) and
               segmentation of tissue without artifacts (Dice score 0.919-0.938
               dependent on magnification). Slides from 19 international
               pathology departments digitized with the most common scanning
               systems and from The Cancer Genome Atlas dataset were used to
               establish a QC benchmark, analyzing inter-institutional,
               intra-institutional, temporal, and inter-scanner slide quality
               variations. GrandQC improves the performance of downstream image
               analysis algorithms. We open-source the GrandQC tool, our large
               manually annotated test dataset, and all QC masks for the entire
               TCGA cohort to address the problem of QC in digital/computational
               pathology. GrandQC can be used as a tool to monitor sample
               preparation and scanning quality in pathology departments and
               help to track and eliminate major artifact sources.",
  month     =  "16~" # dec,
  year      =  2024,
  url       = "http://dx.doi.org/10.1038/s41467-024-54769-y",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41467-024-54769-y",
  language  = "en"
}

@ARTICLE{Virshup2023-wl,
  title     = "The scverse project provides a computational ecosystem for
               single-cell omics data analysis",
  author    = "Virshup, Isaac and Bredikhin, Danila and Heumos, Lukas and Palla,
               Giovanni and Sturm, Gregor and Gayoso, Adam and Kats, Ilia and
               Koutrouli, Mikaela and {Scverse Community} and Berger, Bonnie and
               Pe'er, Dana and Regev, Aviv and Teichmann, Sarah A and Finotello,
               Francesca and Wolf, F Alexander and Yosef, Nir and Stegle, Oliver
               and Theis, Fabian J",
  journal   = "Nat. Biotechnol.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  41,
  number    =  5,
  pages     = "604--606",
  month     =  "10~" # may,
  year      =  2023,
  url       = "https://www.nature.com/articles/s41587-023-01733-8",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41587-023-01733-8",
  language  = "en"
}

@ARTICLE{Shaikovski2024-kd,
  title         = "{PRISM}: A multi-modal generative foundation model for
                   slide-level histopathology",
  author        = "Shaikovski, George and Casson, Adam and Severson, Kristen and
                   Zimmermann, Eric and Wang, Yi Kan and Kunz, Jeremy D and
                   Retamero, Juan A and Oakley, Gerard and Klimstra, David and
                   Kanan, Christopher and Hanna, Matthew and Zelechowski, Michal
                   and Viret, Julian and Tenenholtz, Neil and Hall, James and
                   Fusi, Nicolo and Yousfi, Razik and Hamilton, Peter and Moye,
                   William A and Vorontsov, Eugene and Liu, Siqi and Fuchs,
                   Thomas J",
  journal       = "arXiv [eess.IV]",
  abstract      = "Foundation models in computational pathology promise to
                   unlock the development of new clinical decision support
                   systems and models for precision medicine. However, there is
                   a mismatch between most clinical analysis, which is defined
                   at the level of one or more whole slide images, and
                   foundation models to date, which process the thousands of
                   image tiles contained in a whole slide image separately. The
                   requirement to train a network to aggregate information
                   across a large number of tiles in multiple whole slide images
                   limits these models' impact. In this work, we present a
                   slide-level foundation model for H\&E-stained histopathology,
                   PRISM, that builds on Virchow tile embeddings and leverages
                   clinical report text for pre-training. Using the tile
                   embeddings, PRISM produces slide-level embeddings with the
                   ability to generate clinical reports, resulting in several
                   modes of use. Using text prompts, PRISM achieves zero-shot
                   cancer detection and sub-typing performance approaching and
                   surpassing that of a supervised aggregator model. Using the
                   slide embeddings with linear classifiers, PRISM surpasses
                   supervised aggregator models. Furthermore, we demonstrate
                   that fine-tuning of the PRISM slide encoder yields
                   label-efficient training for biomarker prediction, a task
                   that typically suffers from low availability of training
                   data; an aggregator initialized with PRISM and trained on as
                   little as 10\% of the training data can outperform a
                   supervised baseline that uses all of the data.",
  month         =  "16~" # may,
  year          =  2024,
  url           = "http://arxiv.org/abs/2405.10254",
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Ding2024-pk,
  title         = "Multimodal whole slide foundation model for pathology",
  author        = "Ding, Tong and Wagner, Sophia J and Song, Andrew H and Chen,
                   Richard J and Lu, Ming Y and Zhang, Andrew and Vaidya, Anurag
                   J and Jaume, Guillaume and Shaban, Muhammad and Kim, Ahrong
                   and Williamson, Drew F K and Chen, Bowen and Almagro-Perez,
                   Cristina and Doucet, Paul and Sahai, Sharifa and Chen,
                   Chengkuan and Komura, Daisuke and Kawabe, Akihiro and
                   Ishikawa, Shumpei and Gerber, Georg and Peng, Tingying and
                   Le, Long Phi and Mahmood, Faisal",
  journal       = "arXiv [eess.IV]",
  abstract      = "The field of computational pathology has been transformed
                   with recent advances in foundation models that encode
                   histopathology region-of-interests (ROIs) into versatile and
                   transferable feature representations via self-supervised
                   learning (SSL). However, translating these advancements to
                   address complex clinical challenges at the patient and slide
                   level remains constrained by limited clinical data in
                   disease-specific cohorts, especially for rare clinical
                   conditions. We propose TITAN, a multimodal whole slide
                   foundation model pretrained using 335,645 WSIs via visual
                   self-supervised learning and vision-language alignment with
                   corresponding pathology reports and 423,122 synthetic
                   captions generated from a multimodal generative AI copilot
                   for pathology. Without any finetuning or requiring clinical
                   labels, TITAN can extract general-purpose slide
                   representations and generate pathology reports that
                   generalize to resource-limited clinical scenarios such as
                   rare disease retrieval and cancer prognosis. We evaluate
                   TITAN on diverse clinical tasks and find that TITAN
                   outperforms both ROI and slide foundation models across
                   machine learning settings such as linear probing, few-shot
                   and zero-shot classification, rare cancer retrieval and
                   cross-modal retrieval, and pathology report generation.",
  month         =  "29~" # nov,
  year          =  2024,
  url           = "http://arxiv.org/abs/2411.19666",
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  keywords      = "Fellowship;LazySlide-doc-citations"
}

@ARTICLE{Chen2024-qt,
  title     = "Towards a general-purpose foundation model for computational
               pathology",
  author    = "Chen, Richard J and Ding, Tong and Lu, Ming Y and Williamson,
               Drew F K and Jaume, Guillaume and Song, Andrew H and Chen, Bowen
               and Zhang, Andrew and Shao, Daniel and Shaban, Muhammad and
               Williams, Mane and Oldenburg, Lukas and Weishaupt, Luca L and
               Wang, Judy J and Vaidya, Anurag and Le, Long Phi and Gerber,
               Georg and Sahai, Sharifa and Williams, Walt and Mahmood, Faisal",
  journal   = "Nat. Med.",
  publisher = "Nature Publishing Group",
  volume    =  30,
  number    =  3,
  pages     = "850--862",
  abstract  = "Quantitative evaluation of tissue images is crucial for
               computational pathology (CPath) tasks, requiring the objective
               characterization of histopathological entities from whole-slide
               images (WSIs). The high resolution of WSIs and the variability of
               morphological features present significant challenges,
               complicating the large-scale annotation of data for
               high-performance applications. To address this challenge, current
               efforts have proposed the use of pretrained image encoders
               through transfer learning from natural image datasets or
               self-supervised learning on publicly available histopathology
               datasets, but have not been extensively developed and evaluated
               across diverse tissue types at scale. We introduce UNI, a
               general-purpose self-supervised model for pathology, pretrained
               using more than 100 million images from over 100,000 diagnostic
               H\&E-stained WSIs (>77 TB of data) across 20 major tissue types.
               The model was evaluated on 34 representative CPath tasks of
               varying diagnostic difficulty. In addition to outperforming
               previous state-of-the-art models, we demonstrate new modeling
               capabilities in CPath such as resolution-agnostic tissue
               classification, slide classification using few-shot class
               prototypes, and disease subtyping generalization in classifying
               up to 108 cancer types in the OncoTree classification system. UNI
               advances unsupervised representation learning at scale in CPath
               in terms of both pretraining data and downstream evaluation,
               enabling data-efficient artificial intelligence models that can
               generalize and transfer to a wide range of diagnostically
               challenging tasks and clinical workflows in anatomic pathology.",
  month     =  "19~" # mar,
  year      =  2024,
  url       = "https://www.nature.com/articles/s41591-024-02857-3",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41591-024-02857-3",
  language  = "en"
}

@ARTICLE{Xu2024-td,
  title     = "A whole-slide foundation model for digital pathology from
               real-world data",
  author    = "Xu, Hanwen and Usuyama, Naoto and Bagga, Jaspreet and Zhang,
               Sheng and Rao, Rajesh and Naumann, Tristan and Wong, Cliff and
               Gero, Zelalem and González, Javier and Gu, Yu and Xu, Yanbo and
               Wei, Mu and Wang, Wenhui and Ma, Shuming and Wei, Furu and Yang,
               Jianwei and Li, Chunyuan and Gao, Jianfeng and Rosemon, Jaylen
               and Bower, Tucker and Lee, Soohee and Weerasinghe, Roshanthi and
               Wright, Bill J and Robicsek, Ari and Piening, Brian and Bifulco,
               Carlo and Wang, Sheng and Poon, Hoifung",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  630,
  number    =  8015,
  pages     = "181--188",
  abstract  = "Digital pathology poses unique computational challenges, as a
               standard gigapixel slide may comprise tens of thousands of image
               tiles1-3. Prior models have often resorted to subsampling a small
               portion of tiles for each slide, thus missing the important
               slide-level context4. Here we present Prov-GigaPath, a
               whole-slide pathology foundation model pretrained on 1.3 billion
               256 × 256 pathology image tiles in 171,189 whole slides from
               Providence, a large US health network comprising 28 cancer
               centres. The slides originated from more than 30,000 patients
               covering 31 major tissue types. To pretrain Prov-GigaPath, we
               propose GigaPath, a novel vision transformer architecture for
               pretraining gigapixel pathology slides. To scale GigaPath for
               slide-level learning with tens of thousands of image tiles,
               GigaPath adapts the newly developed LongNet5 method to digital
               pathology. To evaluate Prov-GigaPath, we construct a digital
               pathology benchmark comprising 9 cancer subtyping tasks and 17
               pathomics tasks, using both Providence and TCGA data6. With
               large-scale pretraining and ultra-large-context modelling,
               Prov-GigaPath attains state-of-the-art performance on 25 out of
               26 tasks, with significant improvement over the second-best
               method on 18 tasks. We further demonstrate the potential of
               Prov-GigaPath on vision-language pretraining for pathology7,8 by
               incorporating the pathology reports. In sum, Prov-GigaPath is an
               open-weight foundation model that achieves state-of-the-art
               performance on various digital pathology tasks, demonstrating the
               importance of real-world data and whole-slide modelling.",
  month     =  jun,
  year      =  2024,
  url       = "https://www.nature.com/articles/s41586-024-07441-w",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41586-024-07441-w",
  language  = "en"
}

@ARTICLE{Huang2023-wi,
  title    = "A visual-language foundation model for pathology image analysis
              using medical Twitter",
  author   = "Huang, Zhi and Bianchi, Federico and Yuksekgonul, Mert and
              Montine, Thomas J and Zou, James",
  journal  = "Nat. Med.",
  volume   =  29,
  number   =  9,
  pages    = "2307--2316",
  abstract = "The lack of annotated publicly available medical images is a major
              barrier for computational research and education innovations. At
              the same time, many de-identified images and much knowledge are
              shared by clinicians on public forums such as medical Twitter.
              Here we harness these crowd platforms to curate OpenPath, a large
              dataset of 208,414 pathology images paired with natural language
              descriptions. We demonstrate the value of this resource by
              developing pathology language-image pretraining (PLIP), a
              multimodal artificial intelligence with both image and text
              understanding, which is trained on OpenPath. PLIP achieves
              state-of-the-art performances for classifying new pathology images
              across four external datasets: for zero-shot classification, PLIP
              achieves F1 scores of 0.565-0.832 compared to F1 scores of
              0.030-0.481 for previous contrastive language-image pretrained
              model. Training a simple supervised classifier on top of PLIP
              embeddings also achieves 2.5\% improvement in F1 scores compared
              to using other supervised model embeddings. Moreover, PLIP enables
              users to retrieve similar cases by either image or natural
              language search, greatly facilitating knowledge sharing. Our
              approach demonstrates that publicly shared medical information is
              a tremendous resource that can be harnessed to develop medical
              artificial intelligence for enhancing diagnosis, knowledge sharing
              and education.",
  month    =  sep,
  year     =  2023,
  url      = "http://dx.doi.org/10.1038/s41591-023-02504-3",
  keywords = "LazySlide-doc-citations;Fellowship",
  doi      = "10.1038/s41591-023-02504-3",
  language = "en"
}

@ARTICLE{Jaume2024-tq,
  title         = "Multistain pretraining for slide representation learning in
                   pathology",
  author        = "Jaume, Guillaume and Vaidya, Anurag and Zhang, Andrew and
                   Song, Andrew H and Chen, Richard J and Sahai, Sharifa and Mo,
                   Dandan and Madrigal, Emilio and Le, Long Phi and Mahmood,
                   Faisal",
  journal       = "arXiv [eess.IV]",
  abstract      = "Developing self-supervised learning (SSL) models that can
                   learn universal and transferable representations of H\&E
                   gigapixel whole-slide images (WSIs) is becoming increasingly
                   valuable in computational pathology. These models hold the
                   potential to advance critical tasks such as few-shot
                   classification, slide retrieval, and patient stratification.
                   Existing approaches for slide representation learning extend
                   the principles of SSL from small images (e.g., 224 x 224
                   patches) to entire slides, usually by aligning two different
                   augmentations (or views) of the slide. Yet the resulting
                   representation remains constrained by the limited clinical
                   and biological diversity of the views. Instead, we postulate
                   that slides stained with multiple markers, such as
                   immunohistochemistry, can be used as different views to form
                   a rich task-agnostic training signal. To this end, we
                   introduce Madeleine, a multimodal pretraining strategy for
                   slide representation learning. Madeleine is trained with a
                   dual global-local cross-stain alignment objective on large
                   cohorts of breast cancer samples (N=4,211 WSIs across five
                   stains) and kidney transplant samples (N=12,070 WSIs across
                   four stains). We demonstrate the quality of slide
                   representations learned by Madeleine on various downstream
                   evaluations, ranging from morphological and molecular
                   classification to prognostic prediction, comprising 21 tasks
                   using 7,299 WSIs from multiple medical centers. Code is
                   available at https://github.com/mahmoodlab/MADELEINE.",
  month         =  "5~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.02859",
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  keywords      = "LazySlide-doc-citations"
}

@ARTICLE{Andani2025-jq,
  title     = "Histopathology-based protein multiplex generation using deep
               learning",
  author    = "Andani, Sonali and Chen, Boqi and Ficek-Pascual, Joanna and
               Heinke, Simon and Casanova, Ruben and Hild, Bernard Friedrich and
               Sobottka, Bettina and Bodenmiller, Bernd and {Tumor Profiler
               Consortium} and Koelzer, Viktor H and Rätsch, Gunnar",
  journal   = "Nat. Mach. Intell.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  7,
  number    =  8,
  pages     = "1292--1307",
  abstract  = "Multiplexed protein imaging offers valuable insights into
               interactions between tumours and their surrounding tumour
               microenvironment, but its widespread use is limited by cost, time
               and tissue availability. Here we present HistoPlexer, a deep
               learning framework that generates spatially resolved protein
               multiplexes directly from standard haematoxylin and eosin (H\&E)
               histopathology images. HistoPlexer jointly predicts multiple
               tumour and immune markers using a conditional generative
               adversarial architecture with custom loss functions designed to
               ensure pixel- and embedding-level similarity while mitigating
               slice-to-slice variations. A comprehensive evaluation of
               metastatic melanoma samples demonstrates that
               HistoPlexer-generated protein maps closely resemble real maps, as
               validated by expert assessment. They preserve crucial biological
               relationships by capturing spatial co-localization patterns among
               proteins. The spatial distribution of immune infiltration from
               HistoPlexer-generated protein multiplex enables stratification of
               tumours into immune subtypes. In an independent cohort,
               integration of HistoPlexer-derived features into predictive
               models enhances performance in survival prediction and immune
               subtype classification compared to models using H\&E features
               alone. To assess broader applicability, we benchmarked
               HistoPlexer on publicly available pixel-aligned datasets from
               different cancer types. In all settings, HistoPlexer consistently
               outperformed baseline methods, demonstrating robustness across
               diverse tissue types and imaging conditions. By enabling
               whole-slide protein multiplex generation from routine H\&E
               images, HistoPlexer offers a cost- and time-efficient approach to
               tumour microenvironment characterization with strong potential to
               advance precision oncology.",
  month     =  "4~" # aug,
  year      =  2025,
  url       = "http://dx.doi.org/10.1038/s42256-025-01074-y",
  keywords  = "Computational models; Machine learning;
               Melanoma;LazySlide-doc-citations",
  doi       = "10.1038/s42256-025-01074-y",
  language  = "en"
}


@ARTICLE{Wu2025-kv,
  title     = "{ROSIE}: {AI} generation of multiplex immunofluorescence staining
               from histopathology images",
  author    = "Wu, Eric and Bieniosek, Matthew and Wu, Zhenqin and Thakkar,
               Nitya and Charville, Gregory W and Makky, Ahmad and Schürch,
               Christian M and Huyghe, Jeroen R and Peters, Ulrike and Li,
               Christopher I and Li, Li and Giba, Hannah and Behera, Vivek and
               Raman, Arjun and Trevino, Alexandro E and Mayer, Aaron T and Zou,
               James",
  journal   = "Nat. Commun.",
  publisher = "Nature Publishing Group",
  volume    =  16,
  number    =  1,
  pages     =  7633,
  abstract  = "Hematoxylin and eosin (H\&E) is a common and inexpensive
               histopathology assay. Though widely used and information-rich, it
               cannot directly inform about specific molecular markers, which
               require additional experiments to assess. To address this gap, we
               present ROSIE, a deep-learning framework that computationally
               imputes the expression and localization of dozens of proteins
               from H\&E images. Our model is trained on a dataset of over 1300
               paired and aligned H\&E and multiplex immunofluorescence (mIF)
               samples from over a dozen tissues and disease conditions,
               spanning over 16 million cells. Validation of our in silico mIF
               staining method on held-out H\&E samples demonstrates that the
               predicted biomarkers are effective in identifying cell
               phenotypes, particularly distinguishing lymphocytes such as B
               cells and T cells, which are not readily discernible with H\&E
               staining alone. Additionally, ROSIE facilitates the robust
               identification of stromal and epithelial microenvironments and
               immune cell subtypes like tumor-infiltrating lymphocytes (TILs),
               which are important for understanding tumor-immune interactions
               and can help inform treatment strategies in cancer research.",
  month     =  "16~" # aug,
  year      =  2025,
  url       = "http://dx.doi.org/10.1038/s41467-025-62346-0",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41467-025-62346-0",
  language  = "en"
}

@ARTICLE{Xiang2025-fd,
  title     = "A vision-language foundation model for precision oncology",
  author    = "Xiang, Jinxi and Wang, Xiyue and Zhang, Xiaoming and Xi, Yinghua
               and Eweje, Feyisope and Chen, Yijiang and Li, Yuchen and
               Bergstrom, Colin and Gopaulchan, Matthew and Kim, Ted and Yu,
               Kun-Hsing and Willens, Sierra and Olguin, Francesca Maria and
               Nirschl, Jeffrey J and Neal, Joel and Diehn, Maximilian and Yang,
               Sen and Li, Ruijiang",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  638,
  number    =  8051,
  pages     = "769--778",
  abstract  = "Clinical decision-making is driven by multimodal data, including
               clinical notes and pathological characteristics. Artificial
               intelligence approaches that can effectively integrate multimodal
               data hold significant promise in advancing clinical care1,2.
               However, the scarcity of well-annotated multimodal datasets in
               clinical settings has hindered the development of useful models.
               In this study, we developed the Multimodal transformer with
               Unified maSKed modeling (MUSK), a vision-language foundation
               model designed to leverage large-scale, unlabelled, unpaired
               image and text data. MUSK was pretrained on 50 million pathology
               images from 11,577 patients and one billion pathology-related
               text tokens using unified masked modelling. It was further
               pretrained on one million pathology image-text pairs to
               efficiently align the vision and language features. With minimal
               or no further training, MUSK was tested in a wide range of
               applications and demonstrated superior performance across 23
               patch-level and slide-level benchmarks, including image-to-text
               and text-to-image retrieval, visual question answering, image
               classification and molecular biomarker prediction. Furthermore,
               MUSK showed strong performance in outcome prediction, including
               melanoma relapse prediction, pan-cancer prognosis prediction and
               immunotherapy response prediction in lung and gastro-oesophageal
               cancers. MUSK effectively combined complementary information from
               pathology images and clinical reports and could potentially
               improve diagnosis and precision in cancer therapy.",
  month     =  "8~" # feb,
  year      =  2025,
  url       = "http://dx.doi.org/10.1038/s41586-024-08378-w",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41586-024-08378-w",
  language  = "en"
}

@ARTICLE{Adjadj2025-hn,
  title         = "Towards Comprehensive Cellular Characterisation of {H\&E}
                   slides",
  author        = "Adjadj, Benjamin and Bannier, Pierre-Antoine and Horent,
                   Guillaume and Mandela, Sebastien and Lyon, Aurore and
                   Schutte, Kathryn and Marteau, Ulysse and Gaury, Valentin and
                   Dumont, Laura and Mathieu, Thomas and {MOSAIC consortium} and
                   Belbahri, Reda and Schmauch, Benoît and Durand, Eric and Von
                   Loga, Katharina and Gillet, Lucie",
  journal       = "arXiv [cs.CV]",
  abstract      = "Cell detection, segmentation and classification are essential
                   for analyzing tumor microenvironments (TME) on hematoxylin
                   and eosin (H\&E) slides. Existing methods suffer from poor
                   performance on understudied cell types (rare or not present
                   in public datasets) and limited cross-domain generalization.
                   To address these shortcomings, we introduce HistoPLUS, a
                   state-of-the-art model for cell analysis, trained on a novel
                   curated pan-cancer dataset of 108,722 nuclei covering 13 cell
                   types. In external validation across 4 independent cohorts,
                   HistoPLUS outperforms current state-of-the-art models in
                   detection quality by 5.2\% and overall F1 classification
                   score by 23.7\%, while using 5x fewer parameters. Notably,
                   HistoPLUS unlocks the study of 7 understudied cell types and
                   brings significant improvements on 8 of 13 cell types.
                   Moreover, we show that HistoPLUS robustly transfers to two
                   oncology indications unseen during training. To support
                   broader TME biomarker research, we release the model weights
                   and inference code at https://github.com/owkin/histoplus/.",
  month         =  "2~" # sep,
  year          =  2025,
  url           = "http://arxiv.org/abs/2508.09926",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  keywords      = "LazySlide-doc-citations",
  doi           = "10.48550/arXiv.2508.09926"
}

@ARTICLE{Yan2025-nc,
  title     = "{PathOrchestra}: a comprehensive foundation model for
               computational pathology with over 100 diverse clinical-grade
               tasks",
  author    = "Yan, Fang and Wu, Jianfeng and Li, Jiawen and Wang, Wei and Chen,
               Yirong and Wei, Linda and Lu, Jiaxuan and Chen, Wen and Gao,
               Zizhao and Li, Jianan and Li, Heng and Yan, Hong and Ma, Jiabo
               and Chen, Minda and Lu, Yang and Chen, Qing and Wang, Yizhi and
               Ling, Xitong and Wang, Xuenian and Wang, Zihan and Huang, Qiang
               and Hua, Shengyi and Liu, Mianxin and Ma, Lei and Shen, Tian and
               Zhang, Xiaofan and He, Yonghong and Chen, Hao and Zhang, Shaoting
               and Wang, Zhe",
  journal   = "NPJ Digit. Med.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  8,
  number    =  1,
  pages     =  695,
  abstract  = "The complexity and variability of high-resolution pathological
               images present significant challenges in computational pathology.
               While AI-driven pathology foundation models have advanced the
               field, they require large-scale datasets, substantial storage,
               and significant computational resources, as well as rigorous
               validation for clinical applicability. We present PathOrchestra,
               a versatile pathology foundation model trained on 287,424 slides
               from 21 tissue types across three centers. Evaluated on 112 tasks
               from 61 private and 51 public datasets, covering digital slide
               preprocessing, pan-cancer classification, lesion identification,
               multi-cancer subtype classification, biomarker assessment, gene
               expression prediction, and structured report generation. Across
               27,755 whole slide images and 9,415,729 region-of-interest
               images, it achieved over 0.950 accuracy in 47 tasks, including
               pan-cancer classification, lymphoma subtyping, and bladder cancer
               screening. It is the first to generate structured reports for
               colorectal cancer and lymphoma. Overall, PathOrchestra
               demonstrates the clinical readiness of large-scale
               self-supervised pathology foundation models, achieving high
               accuracy and offering potential to digital medicine integration.",
  month     =  "19~" # nov,
  year      =  2025,
  url       = "http://dx.doi.org/10.1038/s41746-025-02027-w",
  keywords  = "LazySlide-doc-citations",
  doi       = "10.1038/s41746-025-02027-w",
  language  = "en"
}

@ARTICLE{Sellergren2025-qq,
  title         = "{MedGemma} Technical Report",
  author        = "Sellergren, Andrew and Kazemzadeh, Sahar and Jaroensri, Tiam
                   and Kiraly, Atilla and Traverse, Madeleine and Kohlberger,
                   Timo and Xu, Shawn and Jamil, Fayaz and Hughes, Cían and Lau,
                   Charles and Chen, Justin and Mahvar, Fereshteh and Yatziv,
                   Liron and Chen, Tiffany and Sterling, Bram and Baby, Stefanie
                   Anna and Baby, Susanna Maria and Lai, Jeremy and Schmidgall,
                   Samuel and Yang, Lu and Chen, Kejia and Bjornsson, Per and
                   Reddy, Shashir and Brush, Ryan and Philbrick, Kenneth and
                   Asiedu, Mercy and Mezerreg, Ines and Hu, Howard and Yang,
                   Howard and Tiwari, Richa and Jansen, Sunny and Singh, Preeti
                   and Liu, Yun and Azizi, Shekoofeh and Kamath, Aishwarya and
                   Ferret, Johan and Pathak, Shreya and Vieillard, Nino and
                   Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana
                   and Ramé, Alexandre and Riviere, Morgane and Rouillard, Louis
                   and Mesnard, Thomas and Cideron, Geoffrey and Grill,
                   Jean-Bastien and Ramos, Sabela and Yvinec, Edouard and
                   Casbon, Michelle and Buchatskaya, Elena and Alayrac,
                   Jean-Baptiste and Lepikhin, Dmitry and Feinberg, Vlad and
                   Borgeaud, Sebastian and Andreev, Alek and Hardin, Cassidy and
                   Dadashi, Robert and Hussenot, Léonard and Joulin, Armand and
                   Bachem, Olivier and Matias, Yossi and Chou, Katherine and
                   Hassidim, Avinatan and Goel, Kavi and Farabet, Clement and
                   Barral, Joelle and Warkentin, Tris and Shlens, Jonathon and
                   Fleet, David and Cotruta, Victor and Sanseviero, Omar and
                   Martins, Gus and Kirk, Phoebe and Rao, Anand and Shetty,
                   Shravya and Steiner, David F and Kirmizibayrak, Can and
                   Pilgrim, Rory and Golden, Daniel and Yang, Lin",
  journal       = "arXiv [cs.AI]",
  abstract      = "Artificial intelligence (AI) has significant potential in
                   healthcare applications, but its training and deployment
                   faces challenges due to healthcare's diverse data, complex
                   tasks, and the need to preserve privacy. Foundation models
                   that perform well on medical tasks and require less
                   task-specific tuning data are critical to accelerate the
                   development of healthcare AI applications. We introduce
                   MedGemma, a collection of medical vision-language foundation
                   models based on Gemma 3 4B and 27B. MedGemma demonstrates
                   advanced medical understanding and reasoning on images and
                   text, significantly exceeding the performance of
                   similar-sized generative models and approaching the
                   performance of task-specific models, while maintaining the
                   general capabilities of the Gemma 3 base models. For
                   out-of-distribution tasks, MedGemma achieves 2.6-10\%
                   improvement on medical multimodal question answering,
                   15.5-18.1\% improvement on chest X-ray finding
                   classification, and 10.8\% improvement on agentic evaluations
                   compared to the base models. Fine-tuning MedGemma further
                   improves performance in subdomains, reducing errors in
                   electronic health record information retrieval by 50\% and
                   reaching comparable performance to existing specialized
                   state-of-the-art methods for pneumothorax classification and
                   histopathology patch classification. We additionally
                   introduce MedSigLIP, a medically-tuned vision encoder derived
                   from SigLIP. MedSigLIP powers the visual understanding
                   capabilities of MedGemma and as an encoder achieves
                   comparable or better performance than specialized medical
                   image encoders. Taken together, the MedGemma collection
                   provides a strong foundation of medical image and text
                   capabilities, with potential to significantly accelerate
                   medical research and development of downstream applications.
                   The MedGemma collection, including tutorials and model
                   weights, can be found at https://goo.gle/medgemma.",
  month         =  "12~" # jul,
  year          =  2025,
  url           = "http://arxiv.org/abs/2507.05201",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  keywords      = "LazySlide-doc-citations",
  doi           = "10.48550/arXiv.2507.05201"
}

